{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Themes: UCB Fake News Detector\n",
    "## Group: Hollis lab group 2\n",
    "### Members: Shuheng Liu, Qiaoyi Yin, Yuyuan  Fang\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Set Up\n",
    "#### Tools for preprocessing the raw data.\n",
    "(with tools.DocumentSequence and tools.DocumentEmbedder provided by Shuheng Liu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # Ignore some unimportant warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import gensim\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import chain\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel, Word2Vec, Doc2Vec, KeyedVectors\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "# a very popular graph plotting library \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn\n",
    "%matplotlib inline\n",
    "\n",
    "# in case some packages are not properly installed\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('reuters')\n",
    "nltk.download('stopwords')\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "from tools import DocumentSequence,DocumentEmbedder\n",
    "\n",
    "## Save and load file\n",
    "import pickle as pkl\n",
    "\n",
    "def get_file(path):\n",
    "    print('Trying to load file at:{}'.format(path))\n",
    "    try:\n",
    "        with open(path, \"rb\") as f:\n",
    "            item = pkl.load(f)\n",
    "    except FileNotFoundError as e:\n",
    "        print(\"unable to load {}, see stack trace below\".format(path))\n",
    "        print(\"double check that you have the file saved {}\".format(path))\n",
    "        print(e)\n",
    "        return None\n",
    "    print('Loading success')\n",
    "    return item\n",
    "\n",
    "def save_file(path, file):\n",
    "    with open(path,\"wb\") as f:\n",
    "        print(\"Storing item in {}\".format(save_embeddings_path))\n",
    "        pkl.dump(file,f)\n",
    "        print(\"Item stored\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the dataset into pandas: fake_or_real_news.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>title_vectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>1</td>\n",
       "      <td>[ 1.1533764e-02  4.2144405e-03  1.9692603e-02 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>1</td>\n",
       "      <td>[ 0.11267698  0.02518966 -0.00212591  0.021095...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>0</td>\n",
       "      <td>[ 0.04253004  0.04300297  0.01848392  0.048672...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>1</td>\n",
       "      <td>[ 0.10801624  0.11583211  0.02874823  0.061732...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>0</td>\n",
       "      <td>[ 1.69016439e-02  7.13498285e-03 -7.81233795e-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0        8476                       You Can Smell Hillary’s Fear   \n",
       "1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2        3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3       10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4         875   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text label  \\\n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...     1   \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...     1   \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...     0   \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...     1   \n",
       "4  It's primary day in New York and front-runners...     0   \n",
       "\n",
       "                                       title_vectors  \n",
       "0  [ 1.1533764e-02  4.2144405e-03  1.9692603e-02 ...  \n",
       "1  [ 0.11267698  0.02518966 -0.00212591  0.021095...  \n",
       "2  [ 0.04253004  0.04300297  0.01848392  0.048672...  \n",
       "3  [ 0.10801624  0.11583211  0.02874823  0.061732...  \n",
       "4  [ 1.69016439e-02  7.13498285e-03 -7.81233795e-...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "# load the raw data set and Google pretrained w2v model\n",
    "df = pd.read_csv(\"./fake_or_real_news.csv\")\n",
    "pretrained = \"./pretrained/GoogleNews-vectors-negative300.bin\"\n",
    "\n",
    "# obtain the raw label data    {'FAKE':1, 'REAL':0}\n",
    "def trans_labels(labels):           \n",
    "    for idx in range(len(labels)):\n",
    "        if labels[idx] == 'FAKE':\n",
    "            labels[idx] = 1\n",
    "        else:\n",
    "            labels[idx] = 0\n",
    "    return np.array(labels, dtype=int)\n",
    "\n",
    "labels = trans_labels(df['label'].values)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize and clean raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting raw docs into tokens\n",
      "cleaning up stopwords and punctuations\n",
      "all tokens to be skipped are: {'why', 'didn', 'he', 'mustn', 'did', 'being', 'hadn', 'yours', 'whom', 'same', '^', '=', 'too', 'not', 'from', 'again', 'then', 'ours', ';', 'they', 'in', 'aren', \"didn't\", \"you're\", 'them', '!', '/', 'during', 'between', \"weren't\", 'should', 'doing', 'y', 'through', 'the', 'because', \"shan't\", 'up', 'isn', 't', \"hadn't\", 'me', 'herself', 'than', 'just', 'both', 'doesn', 'yourselves', 'with', 'we', \"you'll\", 'any', '\"', '>', 'she', \"you'd\", 'its', 'over', 'm', 'now', 'which', 'where', \"couldn't\", '+', 'an', 'shouldn', 'shan', \"wouldn't\", 'very', 'their', 'll', '<', 'have', 'there', 'these', '|', ',', 'will', 'after', \"wasn't\", 'was', 'once', 'nor', \"mustn't\", 'does', 'his', \"doesn't\", 'that', '&', 'into', 'my', 'by', 'this', 'when', 'as', 'needn', 'her', 's', 'under', 'ain', 'down', \"aren't\", '-', ':', '`', 'can', 'theirs', 'i', 'be', 'won', 'of', 'ourselves', \"you've\", 'before', 'some', 'but', \"it's\", 'hasn', 'your', '}', \"don't\", 'here', 'do', 'himself', 'for', 'to', 'ma', '#', 'yourself', 'a', '[', 'are', 'myself', 'and', 'off', 'further', 'about', 'only', \"haven't\", 'has', 'who', 'am', 'against', '(', '?', 'themselves', 'all', 've', 'had', 'couldn', '$', 'you', 're', ')', 'our', 'so', 'd', 'wasn', 'those', 'most', \"hasn't\", 'above', '.', 'hers', 'until', 'having', \"'\", 'it', 'if', 'each', \"she's\", 'haven', 'no', \"isn't\", 'or', 'few', \"should've\", \"shouldn't\", 'what', 'is', 'such', \"mightn't\", 'don', \"that'll\", 'other', ']', 'how', 'been', \"won't\", 'weren', 'wouldn', '{', 'out', 'below', '_', 'while', 'him', 'own', 'itself', 'on', \"needn't\", '%', 'more', '*', 'mightn', 'were', 'at', '~', '\\\\', 'o', '@'}\n",
      "listing tagged documents in memory\n",
      "converting raw docs into tokens\n",
      "cleaning up stopwords and punctuations\n",
      "all tokens to be skipped are: {'why', 'didn', 'he', 'mustn', 'did', 'being', 'hadn', 'yours', 'whom', 'same', '^', '=', 'too', 'not', 'from', 'again', 'then', 'ours', ';', 'they', 'in', 'aren', \"didn't\", \"you're\", 'them', '!', '/', 'during', 'between', \"weren't\", 'should', 'doing', 'y', 'through', 'the', 'because', \"shan't\", 'up', 'isn', 't', \"hadn't\", 'me', 'herself', 'than', 'just', 'both', 'doesn', 'yourselves', 'with', 'we', \"you'll\", 'any', '\"', '>', 'she', \"you'd\", 'its', 'over', 'm', 'now', 'which', 'where', \"couldn't\", '+', 'an', 'shouldn', 'shan', \"wouldn't\", 'very', 'their', 'll', '<', 'have', 'there', 'these', '|', ',', 'will', 'after', \"wasn't\", 'was', 'once', 'nor', \"mustn't\", 'does', 'his', \"doesn't\", 'that', '&', 'into', 'my', 'by', 'this', 'when', 'as', 'needn', 'her', 's', 'under', 'ain', 'down', \"aren't\", '-', ':', '`', 'can', 'theirs', 'i', 'be', 'won', 'of', 'ourselves', \"you've\", 'before', 'some', 'but', \"it's\", 'hasn', 'your', '}', \"don't\", 'here', 'do', 'himself', 'for', 'to', 'ma', '#', 'yourself', 'a', '[', 'are', 'myself', 'and', 'off', 'further', 'about', 'only', \"haven't\", 'has', 'who', 'am', 'against', '(', '?', 'themselves', 'all', 've', 'had', 'couldn', '$', 'you', 're', ')', 'our', 'so', 'd', 'wasn', 'those', 'most', \"hasn't\", 'above', '.', 'hers', 'until', 'having', \"'\", 'it', 'if', 'each', \"she's\", 'haven', 'no', \"isn't\", 'or', 'few', \"should've\", \"shouldn't\", 'what', 'is', 'such', \"mightn't\", 'don', \"that'll\", 'other', ']', 'how', 'been', \"won't\", 'weren', 'wouldn', '{', 'out', 'below', '_', 'while', 'him', 'own', 'itself', 'on', \"needn't\", '%', 'more', '*', 'mightn', 'were', 'at', '~', '\\\\', 'o', '@'}\n",
      "listing tagged documents in memory\n",
      "Storing item in ./pretrained/title_text-d2v(vecsize=300, winsize=13, mincount=5, dbow, epochs=100).pkl\n",
      "Item stored\n",
      "Storing item in ./pretrained/title_text-d2v(vecsize=300, winsize=13, mincount=5, dbow, epochs=100).pkl\n",
      "Item stored\n"
     ]
    }
   ],
   "source": [
    "# obtain the raw news texts and titles\n",
    "raw_texts = df['text'].values\n",
    "raw_title = df['title'].values\n",
    "\n",
    "# build two instances for preprocessing raw data\n",
    "texts = DocumentSequence(raw_texts, clean=True, sw=stopwords.words('english'), punct=punctuation)\n",
    "titles = DocumentSequence(raw_title, clean=True, sw=stopwords.words('english'), punct=punctuation)\n",
    "\n",
    "# save these two data\n",
    "save_file('./pretrained/texts.pkl',texts)\n",
    "save_file('./pretrained/titles.pkl',titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get embeddings\n",
    "#### How do we get embeddings:\n",
    "1. Text:   \n",
    "\n",
    "| Embeddings | Parameters | \n",
    "| ------ | ------ | \n",
    "| Dov2Vec | (Min_count = 5,Winsize = 13, DBOW/DM) | \n",
    "| Naive Doc2Vec | Normalizer = L2/Mean/None | \n",
    "| One-Hot Sum |(Rawcount/TF-IDF, Normalized/None) | \n",
    "| Attention is all you need | To be implemented |\n",
    "| FastText | To be implemented |\n",
    "            \n",
    "2. Title:   \n",
    "    .....  \n",
    "3. Title concatenated with Text: Concatenate Title_d2v and Text_d2v together(for D2V)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build two instances for producing document embeddings\n",
    "text_embedder = DocumentEmbedder(texts, pretrained_word2vec=pretrained)\n",
    "titles_embedder = DocumentEmbedder(titles, pretrained_word2vec=pretrained)\n",
    "\n",
    "# vectors_size: Number of dimensions for the embedding model\n",
    "# window: Number of context words to observe in each direction within a document\n",
    "# min_count: Minimum frequency for words included in model\n",
    "# dm (distributed memory): '0' indicates DBOW model; '1' indicates DM\n",
    "# epoches: Number of epochs to train the model for\n",
    "text_embeddings = text_embedder.get_doc2vec(vectors_size=300,\n",
    "                                            window=13,\n",
    "                                            min_count=5,\n",
    "                                            dm=0,\n",
    "                                            epochs=100)\n",
    "\n",
    "title_embeddings = titles_embedder.get_doc2vec(vectors_size=300,\n",
    "                                               window=13,\n",
    "                                               min_count=5,\n",
    "                                               dm=0,\n",
    "                                               epochs=100)\n",
    "\n",
    "# if the embeddings is in a list, stack them into a 2-D numpy array\n",
    "def trans_list_to_array(embeddings):\n",
    "    if isinstance(embeddings, list): \n",
    "        try:\n",
    "            embeddings = np.stack(emb if isinstance(emb, np.ndarray) else np.zeros(300) for emb in embeddings)\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "    return embeddings\n",
    "\n",
    "# change text_embeddings and title_embeddings into 2-D numpy array\n",
    "text_embeddings = trans_list_to_array(text_embeddings)\n",
    "title_embeddings = trans_list_to_array(title_embeddings)\n",
    "\n",
    "# concatenate text matrix and title matrix as a whole for training\n",
    "news_embeddings = np.concatenate((title_embeddings, text_embeddings), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing item in ./pretrained/title_text-d2v(vecsize=300, winsize=13, mincount=5, dbow, epochs=100).pkl\n",
      "Item stored\n",
      "Trying to load file at:./pretrained/text-d2v(vecsize=300, winsize=13, mincount=5, dbow, epochs=100).pkl\n",
      "Loading success\n",
      "Trying to load file at:./pretrained/title-d2v(vecsize=300, winsize=13, mincount=5, dbow, epochs=100).pkl\n",
      "Loading success\n",
      "Trying to load file at:./pretrained/title_text-d2v(vecsize=300, winsize=13, mincount=5, dbow, epochs=100).pkl\n",
      "Loading success\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "import os\n",
    "\n",
    "# store the d2v model in files\n",
    "save_embeddings_path_tail = \"d2v(vecsize={}, winsize={}, mincount={}, {}, epochs={}).pkl\".format(\n",
    "    300, 13, 5, \"dbow\", 100)\n",
    "\n",
    "# store the text_embeddings in files\n",
    "save_embeddings_path = \"./pretrained/text-\" + save_embeddings_path_tail\n",
    "save_file(save_embeddings_path,text_embeddings)\n",
    "\n",
    "# store the title_embeddings in files\n",
    "save_embeddings_path = \"./pretrained/title-\" + save_embeddings_path_tail\n",
    "save_file(save_embeddings_path,title_embeddings)\n",
    "\n",
    "# store the text_embeddings in files\n",
    "save_embeddings_path = \"./pretrained/title_text-\" + save_embeddings_path_tail\n",
    "save_file(save_embeddings_path,news_embeddings)\n",
    "\n",
    "# store the labels in files\n",
    "save_labels_path = \"./pretrained/labels.pkl\"\n",
    "save_file(save_labels_path,labels)\n",
    "\n",
    "# # store the d2v model in files\n",
    "# save_embeddings_path_tail = \"d2v(vecsize={}, winsize={}, mincount={}, {}, epochs={}).pkl\".format(\n",
    "#     300, 13, 5, \"dbow\", 100)\n",
    "\n",
    "# # get the text_embeddings in files\n",
    "# save_embeddings_path = \"./pretrained/text-\" + save_embeddings_path_tail\n",
    "# text_embeddings = get_file(save_embeddings_path)\n",
    "\n",
    "# # get the title_embeddings in files\n",
    "# save_embeddings_path = \"./pretrained/title-\" + save_embeddings_path_tail\n",
    "# title_embeddings = get_file(save_embeddings_path)\n",
    "\n",
    "# # get the text_embeddings in files\n",
    "# save_embeddings_path = \"./pretrained/title_text-\" + save_embeddings_path_tail\n",
    "# news_embeddings = get_file(save_embeddings_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the news embeddings\n",
    "(with visualize_embeddings.embedding_visualizer provided by Shuheng Liu. \n",
    "Tensorflow needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: potential error due to tensorboard version conflicts\n",
      "currently setting metadata_path to metadata.tsv. Due to tensorboard version reasons, if prompted 'metadata not found' when visiting tensorboard server page, please manually edit metadata_path in projector_config.pbtxt to visual\\metadata.tsv or the absolute path for `metadata.tsv` and restart tensorboard\n",
      "If your tensorboard version is 1.7.0, you probably should not worry about this\n",
      "Embeddings are available now. Please start your tensorboard server with commandline `tensorboard --logdir visual` and visit http://localhost:6006 to see the visualization\n"
     ]
    }
   ],
   "source": [
    "from embedding_visualizer import visualize_embeddings\n",
    "\n",
    "# visualize the news embeddings in the graph\n",
    "# MUST run in command line \"tensorboard --logdir visual/\" and visit localhost:6006 to see the visualization\n",
    "visualize_embeddings(embedding_values=news_embeddings, label_values=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir visual/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D visualizing \n",
    "- Red:Fake \n",
    "- Blue:Real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![a](resources/T-SNE_2D.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D visualizing \n",
    "- Red:Fake \n",
    "- Blue:Real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![a](resources/T-SNE_3D.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Doc2Vec:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the dataset (with 75% of data for 5-fold Randomsearching, 25% for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to load file at:./pretrained/title_text-d2v(vecsize=300, winsize=13, mincount=5, dbow, epochs=100).pkl\n",
      "Loading success\n",
      "Trying to load file at:./pretrained/labels.pkl\n",
      "Loading success\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.model_selection._search import BaseSearchCV\n",
    "import pickle as pkl\n",
    "\n",
    "# load pretrained data\n",
    "news_embeddings = get_file(\"./pretrained/title_text-d2v(vecsize=300, winsize=13, mincount=5, dbow, epochs=100).pkl\")\n",
    "labels = get_file(\"./pretrained/labels.pkl\")\n",
    "\n",
    "# perform the split which gets us the train data and the test data\n",
    "news_train, news_test, labels_train, labels_test = train_test_split(news_embeddings, labels,\n",
    "                                                                    test_size=0.25,\n",
    "                                                                    random_state=0,\n",
    "                                                                    stratify=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier score and comparement \n",
    "We used RandomSearch on different datasets to get the best hyper-parameters.    \n",
    "The following exhibits every classifier with almost optimal parameters in our experiments.   \n",
    "The RandomSearch process is omitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from scipy.stats import randint\n",
    "from scipy.stats.distributions import uniform\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "# MLP classifier\n",
    "mlp = MLPClassifier(activation='relu', alpha=0.01, batch_size='auto', beta_1=0.8,\n",
    "                    beta_2=0.9, early_stopping=False, epsilon=1e-08,\n",
    "                    hidden_layer_sizes=(600, 300), learning_rate='constant',\n",
    "                    learning_rate_init=0.0001, max_iter=200, momentum=0.9,\n",
    "                    nesterovs_momentum=True, power_t=0.5, random_state=0, shuffle=True,\n",
    "                    solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
    "                    warm_start=False)\n",
    "\n",
    "# KNN classifier\n",
    "knn = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='cosine',\n",
    "                           metric_params=None, n_jobs=1, n_neighbors=10, p=2,\n",
    "                           weights='distance')\n",
    "\n",
    "# QDA classifier\n",
    "qda = QuadraticDiscriminantAnalysis(priors=np.array([0.5, 0.5]),\n",
    "                                    reg_param=0.6531083254653984, store_covariance=False,\n",
    "                                    store_covariances=None, tol=0.0001)\n",
    "\n",
    "# GDB classifier\n",
    "gdb = GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
    "                                 learning_rate=0.1, loss='exponential', max_depth=10,\n",
    "                                 max_features='log2', max_leaf_nodes=None,\n",
    "                                 min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                                 min_samples_leaf=0.0012436966435001434,\n",
    "                                 min_samples_split=100, min_weight_fraction_leaf=0.0,\n",
    "                                 n_estimators=200, presort='auto', random_state=0,\n",
    "                                 subsample=0.8, verbose=0, warm_start=False)\n",
    "\n",
    "# SVC classifier\n",
    "svc = SVC(C=0.8, cache_size=200, class_weight=None, coef0=0.0,\n",
    "          decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
    "          max_iter=-1, probability=False, random_state=0, shrinking=True,\n",
    "          tol=0.001, verbose=False)\n",
    "\n",
    "# GNB classifier\n",
    "gnb = GaussianNB(priors=None)\n",
    "\n",
    "# RF classifier\n",
    "rf = RandomForestClassifier(bootstrap=False, class_weight=None,\n",
    "                            criterion='entropy', max_depth=10, max_features=7,\n",
    "                            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                            min_impurity_split=None, min_samples_leaf=9,\n",
    "                            min_samples_split=6, min_weight_fraction_leaf=0.0,\n",
    "                            n_estimators=50, n_jobs=1, oob_score=False, random_state=None,\n",
    "                            verbose=0, warm_start=False)\n",
    "\n",
    "\n",
    "# All the parameters of the classifiers above are optimal in our experiments\n",
    "# The list below is used to store every classifier instance\n",
    "classifiers_list = [mlp, knn, qda, gdb, svc, gnb, rf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of scores achieved by different classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![a](resources/models_with_best_performance.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(activation='relu', alpha=0.01, batch_size='auto', beta_1=0.8,\n",
      "       beta_2=0.9, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(600, 300), learning_rate='constant',\n",
      "       learning_rate_init=0.0001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=0, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Fake      0.944     0.932     0.938       791\n",
      "       Real      0.933     0.945     0.939       793\n",
      "\n",
      "avg / total      0.938     0.938     0.938      1584\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='cosine',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=10, p=2,\n",
      "           weights='distance')\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Fake      0.907     0.814     0.858       791\n",
      "       Real      0.832     0.917     0.872       793\n",
      "\n",
      "avg / total      0.869     0.866     0.865      1584\n",
      "\n",
      "QuadraticDiscriminantAnalysis(priors=array([0.5, 0.5]),\n",
      "               reg_param=0.6531083254653984, store_covariance=False,\n",
      "               store_covariances=None, tol=0.0001)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Fake      0.960     0.885     0.921       791\n",
      "       Real      0.894     0.963     0.927       793\n",
      "\n",
      "avg / total      0.927     0.924     0.924      1584\n",
      "\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='exponential', max_depth=10,\n",
      "              max_features='log2', max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=0.0012436966435001434,\n",
      "              min_samples_split=100, min_weight_fraction_leaf=0.0,\n",
      "              n_estimators=200, presort='auto', random_state=0,\n",
      "              subsample=0.8, verbose=0, warm_start=False)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Fake      0.880     0.893     0.886       791\n",
      "       Real      0.891     0.879     0.885       793\n",
      "\n",
      "avg / total      0.886     0.886     0.886      1584\n",
      "\n",
      "SVC(C=0.8, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=0, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Fake      0.934     0.932     0.933       791\n",
      "       Real      0.932     0.934     0.933       793\n",
      "\n",
      "avg / total      0.933     0.933     0.933      1584\n",
      "\n",
      "GaussianNB(priors=None)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Fake      0.784     0.874     0.827       791\n",
      "       Real      0.858     0.760     0.806       793\n",
      "\n",
      "avg / total      0.821     0.817     0.816      1584\n",
      "\n",
      "RandomForestClassifier(bootstrap=False, class_weight=None,\n",
      "            criterion='entropy', max_depth=10, max_features=7,\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=9,\n",
      "            min_samples_split=6, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=50, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Fake      0.782     0.867     0.823       791\n",
      "       Real      0.851     0.759     0.803       793\n",
      "\n",
      "avg / total      0.817     0.813     0.813      1584\n",
      "\n",
      "LogisticRegression(C=74.9222112826074, class_weight=None, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Fake      0.914     0.917     0.915       791\n",
      "       Real      0.917     0.914     0.915       793\n",
      "\n",
      "avg / total      0.915     0.915     0.915      1584\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# print details of testing results\n",
    "for model in classifiers_list:\n",
    "    model.fit(news_train, labels_train)\n",
    "    labels_pred = model.predict(news_test)\n",
    "    \n",
    "    # Report the metrics\n",
    "    target_names = ['Real', 'Fake']\n",
    "    print(str(model))\n",
    "    print(classification_report(y_true=labels_test, y_pred=labels_pred, target_names=target_names, digits=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
