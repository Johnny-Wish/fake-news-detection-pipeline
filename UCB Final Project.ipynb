{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\86720\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\86720\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\86720\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\86720\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # Ignore some unimportant warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import gensim\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import chain\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel, Word2Vec, Doc2Vec, KeyedVectors\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "# in case some packages are not properly installed\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('reuters')\n",
    "nltk.download('stopwords')\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "class DocumentSequence:\n",
    "    def __init__(self, raw_docs, clean=False, sw=None, punct=None):\n",
    "        \"\"\"\n",
    "        an instance featuring difference representations of a doc sequence\n",
    "\n",
    "        public methods are:\n",
    "            self.get_dictionary()\n",
    "            self.get_tokenized()\n",
    "            self.get_tagged()\n",
    "            self.get_bow()\n",
    "\n",
    "        :param raw_docs: numpy.ndarray[str]\n",
    "            each string for a document\n",
    "        :param clean: bool\n",
    "            whether to clean stopwords and punctuations\n",
    "        :param sw: list[str]\n",
    "            list of stopwords, only works if `clean` is True, default is empty\n",
    "        :param punct: str\n",
    "            string of punctuations, only works if `clean` is True, default is empty\n",
    "\n",
    "        \"\"\"\n",
    "        self.raw_docs = raw_docs\n",
    "        self._set_tokenized(clean=clean, sw=sw, punct=punct)\n",
    "        self._set_tagged()\n",
    "\n",
    "    def _set_tokenized(self, clean=False, sw=None, punct=None):\n",
    "        \"\"\"\n",
    "        set self._tokenized to list[list[str]]: each string for a token\n",
    "        :param clean: bool, whether to clean stopwords and punctuations\n",
    "        :param sw: list[str], list of stopwords, only works if `clean` is True, default is empty\n",
    "        :param punct: str, string of punctuations, only works if `clean` is True, default is empty\n",
    "        \"\"\"\n",
    "        print(\"converting raw docs into tokens\")\n",
    "\n",
    "        # lower-casing all documents in the first step\n",
    "        self._tokenized = [nltk.word_tokenize(doc.lower()) for doc in self.raw_docs]\n",
    "\n",
    "        if clean:  # if clean is set to True, stopwords and punctuations are removed\n",
    "            print(\"cleaning up stopwords and punctuations\")\n",
    "            # hashing stopwords and punctuations speeds up look-up computation\n",
    "            if sw is None:  # default value of sw is None, corresponding to an empty list\n",
    "                sw = []\n",
    "            if punct is None:  # default value of punct is None, corresponding to an empty list\n",
    "                punct = []\n",
    "            skip_tokens = set(chain(sw, punct))\n",
    "            print(\"all tokens to be skipped are: {}\".format(skip_tokens))\n",
    "            # retain only meaningful tokens, while preserving the structure\n",
    "            self._tokenized = [[token for token in doc if token not in skip_tokens] for doc in self._tokenized]\n",
    "\n",
    "    def _set_tagged(self):\n",
    "        \"\"\"set self._set_tagged to list[TaggedDocument] each TaggedDocument has a tag of [index]\"\"\"\n",
    "        print(\"listing tagged documents in memory\")\n",
    "        self._tagged = [TaggedDocument(doc, tags=[index]) for index, doc in enumerate(self._tokenized)]\n",
    "\n",
    "    def _set_dictionary(self):\n",
    "        \"\"\"stores the dictionary of current corpus\"\"\"\n",
    "        self._dictionary = Dictionary(self._tokenized)\n",
    "\n",
    "    def _set_bow(self):\n",
    "        \"\"\"set self._bow to list[list[tuple]], where each tuple is (word_id, word_frequency)\"\"\"\n",
    "        if not hasattr(self, '_dictionary'):  # check whether dictionary is set or not\n",
    "            print(\"dictionary is not set for {}, setting dictionary automatically\".format(self))\n",
    "            self._set_dictionary()\n",
    "        self._bow = [self._dictionary.doc2bow(doc) for doc in self._tokenized]\n",
    "\n",
    "    def get_dictionary(self):\n",
    "        \"\"\"getter for class attribute dictionary\"\"\"\n",
    "        if not hasattr(self, \"_dictionary\"):  # self._dictionary is only computed once\n",
    "            self._set_dictionary()\n",
    "\n",
    "        # the previous method is only called once\n",
    "        return self._dictionary\n",
    "\n",
    "    def get_tokenized(self):\n",
    "        \"\"\"getter for tokenized documents, cleaned as desired\"\"\"\n",
    "        return self._tokenized\n",
    "\n",
    "    def get_tagged(self):\n",
    "        \"\"\"getter for list of TaggedDocuments\"\"\"\n",
    "        return self._tagged\n",
    "\n",
    "    def get_bow(self):\n",
    "        \"\"\"getter for bag-of-words representation of documents\"\"\"\n",
    "        if not hasattr(self, '_bow'):  # self._bow is only computed lazily\n",
    "            self._set_bow()\n",
    "\n",
    "        # the previous method is only called once\n",
    "        return self._bow\n",
    "\n",
    "\n",
    "class DocumentEmbedder:\n",
    "    def __init__(self, docs: DocumentSequence, pretrained_word2vec=None):\n",
    "        \"\"\"\n",
    "        This class features interfaces to different methods of computing document embeddings.\n",
    "        Supported embedding mechanisms are:\n",
    "            Dov2Vec:                               see self.get_doc2vec()\n",
    "            Naive Doc2Vec:                         see self.get_naive_doc2vec()\n",
    "            One-Hot Sum:                           see self.get_onehot()\n",
    "            Attention is all you need              To be implemented\n",
    "            FastText                               To be implemented\n",
    "\n",
    "        :param docs: a DocumentSequence instance\n",
    "        :pretrained_word2vec: path to pretrained word2vec model, in .bin format\n",
    "        \"\"\"\n",
    "        self.docs = docs\n",
    "        self.pretrained = pretrained_word2vec\n",
    "\n",
    "    def _set_word2vec(self):\n",
    "        if self.pretrained is None:\n",
    "            raise ValueError(\"Pretrained word2vec path is not specified during instantiation\")\n",
    "        self._w2v = KeyedVectors.load_word2vec_format(self.pretrained, binary=True)\n",
    "\n",
    "    def _set_doc2vec(self, vector_size=300, window=5, min_count=5, dm=1, epochs=20):\n",
    "        # instantiate a Doc2Vec model, setting pretrained GoogleNews Vector\n",
    "        self._d2v = Doc2Vec(vector_size=vector_size, window=window, min_count=min_count, dm=dm, epochs=epochs,\n",
    "                            pretrained=self.pretrained)\n",
    "        # build vocabulary from corpus\n",
    "        self._d2v.build_vocab(self.docs.get_tagged())\n",
    "\n",
    "        # somehow, the training won't start automatically, and must be manually started\n",
    "        self._d2v.train(self.docs.get_tagged(), total_examples=self._d2v.corpus_count, epochs=epochs)\n",
    "\n",
    "        # list document embeddings by order of their tags\n",
    "        self._d2v_embedding = [self._d2v.docvecs[index]\n",
    "                               for index in range(len(self.docs.get_tagged()))]\n",
    "\n",
    "    def get_doc2vec(self, vectors_size=300, window=5, min_count=5, dm=1, epochs=20):\n",
    "        \"\"\"\n",
    "        get the doc2vec embeddings with word vectors pretrained on GoogleNews task\n",
    "        :param vectors_size: size for document embeddings, should be 300 if using GoogleNews pretrained word vectors\n",
    "        :param window: number of tokens to be include in both directions\n",
    "        :param min_count: lower threshold for a token to be included\n",
    "        :param dm: using distributed memory or not\n",
    "            if 1, use distributed memory\n",
    "            if 0, use distributed bag of words\n",
    "        :param epochs: number of epochs for training, usually < 20\n",
    "        :return: a list of document embeddings, vector size can be specified\n",
    "        \"\"\"\n",
    "        if vectors_size != 300:\n",
    "            print(\"Warning: pretrained Google News vecs have length 300, got vec-size={} \".format(vectors_size))\n",
    "\n",
    "        if not hasattr(self, '_d2v_embedding'):\n",
    "            self._set_doc2vec(vector_size=vectors_size, window=window, min_count=min_count, dm=dm, epochs=epochs)\n",
    "\n",
    "        return self._d2v_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods above are used as tools for preprocessing the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting raw docs into tokens\n",
      "cleaning up stopwords and punctuations\n",
      "all tokens to be skipped are: {'<', 'ma', 'very', '\\\\', 'nor', \"hasn't\", 'ain', 'but', 'myself', 'where', 'off', ',', 'same', 'up', 'no', ';', 'we', \"isn't\", 'each', 'about', 'a', \"'\", 'his', 'them', 'our', 'not', 'your', 'by', 'such', '_', '|', '>', \"you're\", 're', '^', 'am', ':', \"needn't\", \"you'll\", \"she's\", '/', 'me', 'having', 'out', 'again', 'which', 'my', \"you've\", 'this', 'being', 'some', 'with', 'its', 'any', '\"', 'than', 'won', 'itself', 'until', 'shan', \"shan't\", 'further', 'why', 'down', 'too', 't', 'hasn', 'between', 'during', 'i', \"mightn't\", 'that', \"wouldn't\", 'on', '}', '`', \"weren't\", ']', 'an', 'her', 'do', 'now', 'isn', 'have', 'once', 'm', 'those', 'yourself', 'himself', 'can', 'mustn', 'll', 'the', 'aren', 'these', 'ours', 'as', 'here', 'through', 'how', 'most', 'below', 'are', 'over', '@', \"haven't\", 'only', 'she', 'weren', 'of', 'yours', 'is', 'to', 'both', 'shouldn', '&', 'for', 'who', \"don't\", 'y', '?', 'wouldn', 'their', 'while', 'you', '%', 'above', \"didn't\", 'haven', 'has', 'in', \"hadn't\", 'needn', 'before', 'few', 'was', \"won't\", 'wasn', 'doesn', 'so', \"wasn't\", 'and', 'don', 'hadn', 'what', 'they', '.', 'when', 'will', \"mustn't\", '-', '=', \"couldn't\", 'had', 'does', \"you'd\", 'should', \"doesn't\", '$', '(', 'more', 'because', 've', 'just', 'then', 'o', 'mightn', 'it', 'all', 'couldn', '#', 'or', '[', ')', 'at', \"that'll\", 'whom', 'yourselves', 'him', 'herself', 'into', \"shouldn't\", 'hers', 'from', 'ourselves', 'other', 'against', 'own', '+', 'were', '*', 'doing', 'under', \"it's\", 'theirs', 'didn', \"aren't\", '!', 'he', 'd', 's', 'if', 'there', '{', 'after', '~', 'be', \"should've\", 'been', 'themselves', 'did'}\n",
      "listing tagged documents in memory\n",
      "converting raw docs into tokens\n",
      "cleaning up stopwords and punctuations\n",
      "all tokens to be skipped are: {'<', 'ma', 'very', '\\\\', 'nor', \"hasn't\", 'ain', 'but', 'myself', 'where', 'off', ',', 'same', 'up', 'no', ';', 'we', \"isn't\", 'each', 'about', 'a', \"'\", 'his', 'them', 'our', 'not', 'your', 'by', 'such', '_', '|', '>', \"you're\", 're', '^', 'am', ':', \"needn't\", \"you'll\", \"she's\", '/', 'me', 'having', 'out', 'again', 'which', 'my', \"you've\", 'this', 'being', 'some', 'with', 'its', 'any', '\"', 'than', 'won', 'itself', 'until', 'shan', \"shan't\", 'further', 'why', 'down', 'too', 't', 'hasn', 'between', 'during', 'i', \"mightn't\", 'that', \"wouldn't\", 'on', '}', '`', \"weren't\", ']', 'an', 'her', 'do', 'now', 'isn', 'have', 'once', 'm', 'those', 'yourself', 'himself', 'can', 'mustn', 'll', 'the', 'aren', 'these', 'ours', 'as', 'here', 'through', 'how', 'most', 'below', 'are', 'over', '@', \"haven't\", 'only', 'she', 'weren', 'of', 'yours', 'is', 'to', 'both', 'shouldn', '&', 'for', 'who', \"don't\", 'y', '?', 'wouldn', 'their', 'while', 'you', '%', 'above', \"didn't\", 'haven', 'has', 'in', \"hadn't\", 'needn', 'before', 'few', 'was', \"won't\", 'wasn', 'doesn', 'so', \"wasn't\", 'and', 'don', 'hadn', 'what', 'they', '.', 'when', 'will', \"mustn't\", '-', '=', \"couldn't\", 'had', 'does', \"you'd\", 'should', \"doesn't\", '$', '(', 'more', 'because', 've', 'just', 'then', 'o', 'mightn', 'it', 'all', 'couldn', '#', 'or', '[', ')', 'at', \"that'll\", 'whom', 'yourselves', 'him', 'herself', 'into', \"shouldn't\", 'hers', 'from', 'ourselves', 'other', 'against', 'own', '+', 'were', '*', 'doing', 'under', \"it's\", 'theirs', 'didn', \"aren't\", '!', 'he', 'd', 's', 'if', 'there', '{', 'after', '~', 'be', \"should've\", 'been', 'themselves', 'did'}\n",
      "listing tagged documents in memory\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "# load the raw data set and Google pretrained w2v model\n",
    "df = pd.read_csv(\"./fake_or_real_news.csv\")\n",
    "pretrained = \"./pretrained/GoogleNews-vectors-negative300.bin\"\n",
    "# obtain the raw news texts and titles\n",
    "raw_texts = df['text'].values\n",
    "raw_title = df['title'].values\n",
    "\n",
    "# obtain the raw label data\n",
    "def trans_labels(labels):\n",
    "    for idx in range(len(labels)):\n",
    "        if labels[idx] == 'FAKE':\n",
    "            labels[idx] = 0\n",
    "        else:\n",
    "            labels[idx] = 1\n",
    "    return np.array(labels, dtype=int)\n",
    "\n",
    "labels = trans_labels(df['label'].values)\n",
    "\n",
    "# build two instances for preprocessing raw data\n",
    "texts = DocumentSequence(raw_texts, clean=True, sw=stopwords.words('english'), punct=punctuation)\n",
    "titles = DocumentSequence(raw_title, clean=True, sw=stopwords.words('english'), punct=punctuation)\n",
    "\n",
    "# build two instances for producing document embeddings\n",
    "text_embedder = DocumentEmbedder(texts, pretrained_word2vec=pretrained)\n",
    "titles_embedder = DocumentEmbedder(titles, pretrained_word2vec=pretrained)\n",
    "\n",
    "# vectors_size: Number of dimensions for the embedding model\n",
    "# window: Number of context words to observe in each direction within a document\n",
    "# min_count: Minimum frequency for words included in model\n",
    "# dm (distributed memory): '0' indicates DBOW model; '1' indicates DM\n",
    "# epoches: Number of epochs to train the model for\n",
    "text_embeddings = text_embedder.get_doc2vec(vectors_size=300,\n",
    "                                            window=13,\n",
    "                                            min_count=5,\n",
    "                                            dm=0,\n",
    "                                            epochs=100)\n",
    "\n",
    "title_embeddings = titles_embedder.get_doc2vec(vectors_size=300,\n",
    "                                               window=13,\n",
    "                                               min_count=5,\n",
    "                                               dm=0,\n",
    "                                               epochs=100)\n",
    "\n",
    "# if the embeddings is in a list, stack them into a 2-D numpy array\n",
    "def trans_list_to_array(embeddings):\n",
    "    if isinstance(embeddings, list): \n",
    "        try:\n",
    "            embeddings = np.stack(emb if isinstance(emb, np.ndarray) else np.zeros(300) for emb in embeddings)\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "    return embeddings\n",
    "\n",
    "# change text_embeddings and title_embeddings into 2-D numpy array\n",
    "text_embeddings = trans_list_to_array(text_embeddings)\n",
    "title_embeddings = trans_list_to_array(title_embeddings)\n",
    "\n",
    "# concatenate text matrix and title matrix as a whole for training\n",
    "news_embeddings = np.concatenate((title_embeddings, text_embeddings), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process above is used to get embeddings of preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: potential error due to tensorboard version conflicts\n",
      "currently setting metadata_path to metadata.tsv. Due to tensorboard version reasons, if prompted 'metadata not found' when visiting tensorboard server page, please manually edit metadata_path in projector_config.pbtxt to visual\\metadata.tsv or the absolute path for `metadata.tsv` and restart tensorboard\n",
      "If your tensorboard version is 1.7.0, you probably should not worry about this\n",
      "Embeddings are available now. Please start your tensorboard server with commandline `tensorboard --logdir visual` and visit http://localhost:6006 to see the visualization\n"
     ]
    }
   ],
   "source": [
    "from embedding_visualizer import visualize_embeddings\n",
    "\n",
    "# visualize the news embeddings in the graph\n",
    "# MUST run in command line \"tensorboard --logdir visual/\" and visit localhost:6006 to see the visualization\n",
    "visualize_embeddings(embedding_values=news_embeddings, label_values=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method above is used to visualize the news embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "storing embeddings in ./pretrained/title_text-d2v(vecsize=300, winsize=13, mincount=5, dbow, epochs=100).pkl\n",
      "embeddings stored\n",
      "storing labels in ./pretrained/labels.pkl\n",
      "labels stored\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "import os\n",
    "\n",
    "# store the d2v model in files\n",
    "save_embeddings_path = \"d2v(vecsize={}, winsize={}, mincount={}, {}, epochs={}).pkl\".format(\n",
    "    300, 13, 5, \"dbow\", 100)\n",
    "save_embeddings_path = \"./pretrained/title_text-\" + save_embeddings_path\n",
    "# dump the data into files\n",
    "with open(save_embeddings_path, \"wb\") as file:\n",
    "    print(\"storing embeddings in {}\".format(save_embeddings_path))\n",
    "    pkl.dump(news_embeddings, file)\n",
    "    print(\"embeddings stored\")\n",
    "    \n",
    "# store the labels in files\n",
    "save_labels_path = \"./pretrained/labels.pkl\"\n",
    "# dump the data into files\n",
    "with open(save_labels_path, \"wb\") as file:\n",
    "    print(\"storing labels in {}\".format(save_labels_path))\n",
    "    pkl.dump(labels, file)\n",
    "    print(\"labels stored\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process above is used to store d2v embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.model_selection._search import BaseSearchCV\n",
    "import pickle as pkl\n",
    "\n",
    "# obtain the embeddings from files.\n",
    "def get_file(path):\n",
    "    try:\n",
    "        with open(path, \"rb\") as f:\n",
    "            item = pkl.load(f)\n",
    "    except FileNotFoundError as e:\n",
    "        print(\"unable to load {}, see stack trace below\".format(path))\n",
    "        print(\"double check that you have the file saved {}\".format(path))\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "    return item\n",
    "\n",
    "# load pretrained data\n",
    "embeddings = get_file(\"./pretrained/title_text-d2v(vecsize=300, winsize=13, mincount=5, dbow, epochs=100).pkl\")\n",
    "labels = get_file(\"./pretrained/labels.pkl\")\n",
    "\n",
    "# perform the split which gets us the train data and the test data\n",
    "news_train, news_test, labels_train, labels_test = train_test_split(news_embeddings, labels,\n",
    "                                                                    test_size=0.25,\n",
    "                                                                    random_state=0,\n",
    "                                                                    stratify=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process above is used to split the original data into two parts, one for training, the other for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from scipy.stats import randint\n",
    "from scipy.stats.distributions import uniform\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "# MLP classifier\n",
    "mlp = MLPClassifier(activation='relu', alpha=0.01, batch_size='auto', beta_1=0.8,\n",
    "                    beta_2=0.9, early_stopping=False, epsilon=1e-08,\n",
    "                    hidden_layer_sizes=(600, 300), learning_rate='constant',\n",
    "                    learning_rate_init=0.0001, max_iter=200, momentum=0.9,\n",
    "                    nesterovs_momentum=True, power_t=0.5, random_state=0, shuffle=True,\n",
    "                    solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
    "                    warm_start=False)\n",
    "\n",
    "# KNN classifier\n",
    "knn = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='cosine',\n",
    "                           metric_params=None, n_jobs=1, n_neighbors=10, p=2,\n",
    "                           weights='distance')\n",
    "\n",
    "# QDA classifier\n",
    "qda = QuadraticDiscriminantAnalysis(priors=np.array([0.5, 0.5]),\n",
    "                                    reg_param=0.6531083254653984, store_covariance=False,\n",
    "                                    store_covariances=None, tol=0.0001)\n",
    "\n",
    "# GDB classifier\n",
    "gdb = GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
    "                                 learning_rate=0.1, loss='exponential', max_depth=10,\n",
    "                                 max_features='log2', max_leaf_nodes=None,\n",
    "                                 min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                                 min_samples_leaf=0.0012436966435001434,\n",
    "                                 min_samples_split=100, min_weight_fraction_leaf=0.0,\n",
    "                                 n_estimators=200, presort='auto', random_state=0,\n",
    "                                 subsample=0.8, verbose=0, warm_start=False)\n",
    "\n",
    "# SVC classifier\n",
    "svc = SVC(C=0.8, cache_size=200, class_weight=None, coef0=0.0,\n",
    "          decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
    "          max_iter=-1, probability=False, random_state=0, shrinking=True,\n",
    "          tol=0.001, verbose=False)\n",
    "\n",
    "# GNB classifier\n",
    "gnb = GaussianNB(priors=None)\n",
    "\n",
    "# RF classifier\n",
    "rf = RandomForestClassifier(bootstrap=False, class_weight=None,\n",
    "                            criterion='entropy', max_depth=10, max_features=7,\n",
    "                            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                            min_impurity_split=None, min_samples_leaf=9,\n",
    "                            min_samples_split=6, min_weight_fraction_leaf=0.0,\n",
    "                            n_estimators=50, n_jobs=1, oob_score=False, random_state=None,\n",
    "                            verbose=0, warm_start=False)\n",
    "\n",
    "# LGR classifer\n",
    "lgr = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=74.9222112826074,\n",
    "                         fit_intercept=True, intercept_scaling=1,\n",
    "                         class_weight=None, random_state=None,\n",
    "                         solver='liblinear', max_iter=100, multi_class='ovr', \n",
    "                         verbose=0, warm_start=False, n_jobs=1)\n",
    "\n",
    "# All the parameters of the classifiers above are optimal in our experiments\n",
    "# The list below is used to store every classifier instance\n",
    "classifiers_list = [mlp, knn, qda, gdb, svc, gnb, rf, lgr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process above is used to build every classifier with almost optimal parameters in our experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![a](resources/models_with_best_performance.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(activation='relu', alpha=0.01, batch_size='auto', beta_1=0.8,\n",
      "       beta_2=0.9, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(600, 300), learning_rate='constant',\n",
      "       learning_rate_init=0.0001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=0, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Fake      0.944     0.932     0.938       791\n",
      "       Real      0.933     0.945     0.939       793\n",
      "\n",
      "avg / total      0.938     0.938     0.938      1584\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='cosine',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=10, p=2,\n",
      "           weights='distance')\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Fake      0.907     0.814     0.858       791\n",
      "       Real      0.832     0.917     0.872       793\n",
      "\n",
      "avg / total      0.869     0.866     0.865      1584\n",
      "\n",
      "QuadraticDiscriminantAnalysis(priors=array([0.5, 0.5]),\n",
      "               reg_param=0.6531083254653984, store_covariance=False,\n",
      "               store_covariances=None, tol=0.0001)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Fake      0.960     0.885     0.921       791\n",
      "       Real      0.894     0.963     0.927       793\n",
      "\n",
      "avg / total      0.927     0.924     0.924      1584\n",
      "\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='exponential', max_depth=10,\n",
      "              max_features='log2', max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=0.0012436966435001434,\n",
      "              min_samples_split=100, min_weight_fraction_leaf=0.0,\n",
      "              n_estimators=200, presort='auto', random_state=0,\n",
      "              subsample=0.8, verbose=0, warm_start=False)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Fake      0.880     0.893     0.886       791\n",
      "       Real      0.891     0.879     0.885       793\n",
      "\n",
      "avg / total      0.886     0.886     0.886      1584\n",
      "\n",
      "SVC(C=0.8, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=0, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Fake      0.934     0.932     0.933       791\n",
      "       Real      0.932     0.934     0.933       793\n",
      "\n",
      "avg / total      0.933     0.933     0.933      1584\n",
      "\n",
      "GaussianNB(priors=None)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Fake      0.784     0.874     0.827       791\n",
      "       Real      0.858     0.760     0.806       793\n",
      "\n",
      "avg / total      0.821     0.817     0.816      1584\n",
      "\n",
      "RandomForestClassifier(bootstrap=False, class_weight=None,\n",
      "            criterion='entropy', max_depth=10, max_features=7,\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=9,\n",
      "            min_samples_split=6, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=50, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Fake      0.782     0.867     0.823       791\n",
      "       Real      0.851     0.759     0.803       793\n",
      "\n",
      "avg / total      0.817     0.813     0.813      1584\n",
      "\n",
      "LogisticRegression(C=74.9222112826074, class_weight=None, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Fake      0.914     0.917     0.915       791\n",
      "       Real      0.917     0.914     0.915       793\n",
      "\n",
      "avg / total      0.915     0.915     0.915      1584\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# print details of testing results\n",
    "for model in classifiers_list:\n",
    "    model.fit(news_train, labels_train)\n",
    "    labels_pred = model.predict(news_test)\n",
    "    \n",
    "    # Report the metrics\n",
    "    target_names = ['Fake', 'Real']\n",
    "    print(str(model))\n",
    "    print(classification_report(y_true=labels_test, y_pred=labels_pred, target_names=target_names, digits=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
