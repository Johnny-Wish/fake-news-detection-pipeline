{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\86720\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\86720\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\86720\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\86720\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # Ignore some unimportant warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import gensim\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import chain\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel, Word2Vec, Doc2Vec, KeyedVectors\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "# in case some packages are not properly installed\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('reuters')\n",
    "nltk.download('stopwords')\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "class DocumentSequence:\n",
    "    def __init__(self, raw_docs, clean=False, sw=None, punct=None):\n",
    "        \"\"\"\n",
    "        an instance featuring difference representations of a doc sequence\n",
    "\n",
    "        public methods are:\n",
    "            self.get_dictionary()\n",
    "            self.get_tokenized()\n",
    "            self.get_tagged()\n",
    "            self.get_bow()\n",
    "\n",
    "        :param raw_docs: numpy.ndarray[str]\n",
    "            each string for a document\n",
    "        :param clean: bool\n",
    "            whether to clean stopwords and punctuations\n",
    "        :param sw: list[str]\n",
    "            list of stopwords, only works if `clean` is True, default is empty\n",
    "        :param punct: str\n",
    "            string of punctuations, only works if `clean` is True, default is empty\n",
    "\n",
    "        \"\"\"\n",
    "        self.raw_docs = raw_docs\n",
    "        self._set_tokenized(clean=clean, sw=sw, punct=punct)\n",
    "        self._set_tagged()\n",
    "\n",
    "    def _set_tokenized(self, clean=False, sw=None, punct=None):\n",
    "        \"\"\"\n",
    "        set self._tokenized to list[list[str]]: each string for a token\n",
    "        :param clean: bool, whether to clean stopwords and punctuations\n",
    "        :param sw: list[str], list of stopwords, only works if `clean` is True, default is empty\n",
    "        :param punct: str, string of punctuations, only works if `clean` is True, default is empty\n",
    "        \"\"\"\n",
    "        print(\"converting raw docs into tokens\")\n",
    "\n",
    "        # lower-casing all documents in the first step\n",
    "        self._tokenized = [nltk.word_tokenize(doc.lower()) for doc in self.raw_docs]\n",
    "\n",
    "        if clean:  # if clean is set to True, stopwords and punctuations are removed\n",
    "            print(\"cleaning up stopwords and punctuations\")\n",
    "            # hashing stopwords and punctuations speeds up look-up computation\n",
    "            if sw is None:  # default value of sw is None, corresponding to an empty list\n",
    "                sw = []\n",
    "            if punct is None:  # default value of punct is None, corresponding to an empty list\n",
    "                punct = []\n",
    "            skip_tokens = set(chain(sw, punct))\n",
    "            print(\"all tokens to be skipped are: {}\".format(skip_tokens))\n",
    "            # retain only meaningful tokens, while preserving the structure\n",
    "            self._tokenized = [[token for token in doc if token not in skip_tokens] for doc in self._tokenized]\n",
    "\n",
    "    def _set_tagged(self):\n",
    "        \"\"\"set self._set_tagged to list[TaggedDocument] each TaggedDocument has a tag of [index]\"\"\"\n",
    "        print(\"listing tagged documents in memory\")\n",
    "        self._tagged = [TaggedDocument(doc, tags=[index]) for index, doc in enumerate(self._tokenized)]\n",
    "\n",
    "    def _set_dictionary(self):\n",
    "        \"\"\"stores the dictionary of current corpus\"\"\"\n",
    "        self._dictionary = Dictionary(self._tokenized)\n",
    "\n",
    "    def _set_bow(self):\n",
    "        \"\"\"set self._bow to list[list[tuple]], where each tuple is (word_id, word_frequency)\"\"\"\n",
    "        if not hasattr(self, '_dictionary'):  # check whether dictionary is set or not\n",
    "            print(\"dictionary is not set for {}, setting dictionary automatically\".format(self))\n",
    "            self._set_dictionary()\n",
    "        self._bow = [self._dictionary.doc2bow(doc) for doc in self._tokenized]\n",
    "\n",
    "    def get_dictionary(self):\n",
    "        \"\"\"getter for class attribute dictionary\"\"\"\n",
    "        if not hasattr(self, \"_dictionary\"):  # self._dictionary is only computed once\n",
    "            self._set_dictionary()\n",
    "\n",
    "        # the previous method is only called once\n",
    "        return self._dictionary\n",
    "\n",
    "    def get_tokenized(self):\n",
    "        \"\"\"getter for tokenized documents, cleaned as desired\"\"\"\n",
    "        return self._tokenized\n",
    "\n",
    "    def get_tagged(self):\n",
    "        \"\"\"getter for list of TaggedDocuments\"\"\"\n",
    "        return self._tagged\n",
    "\n",
    "    def get_bow(self):\n",
    "        \"\"\"getter for bag-of-words representation of documents\"\"\"\n",
    "        if not hasattr(self, '_bow'):  # self._bow is only computed lazily\n",
    "            self._set_bow()\n",
    "\n",
    "        # the previous method is only called once\n",
    "        return self._bow\n",
    "\n",
    "\n",
    "class DocumentEmbedder:\n",
    "    def __init__(self, docs: DocumentSequence, pretrained_word2vec=None):\n",
    "        \"\"\"\n",
    "        This class features interfaces to different methods of computing document embeddings.\n",
    "        Supported embedding mechanisms are:\n",
    "            Dov2Vec:                               see self.get_doc2vec()\n",
    "            Naive Doc2Vec:                         see self.get_naive_doc2vec()\n",
    "            One-Hot Sum:                           see self.get_onehot()\n",
    "            Attention is all you need              To be implemented\n",
    "            FastText                               To be implemented\n",
    "\n",
    "        :param docs: a DocumentSequence instance\n",
    "        :pretrained_word2vec: path to pretrained word2vec model, in .bin format\n",
    "        \"\"\"\n",
    "        self.docs = docs\n",
    "        self.pretrained = pretrained_word2vec\n",
    "\n",
    "    def _set_word2vec(self):\n",
    "        if self.pretrained is None:\n",
    "            raise ValueError(\"Pretrained word2vec path is not specified during instantiation\")\n",
    "        self._w2v = KeyedVectors.load_word2vec_format(self.pretrained, binary=True)\n",
    "\n",
    "    def _set_doc2vec(self, vector_size=300, window=5, min_count=5, dm=1, epochs=20):\n",
    "        # instantiate a Doc2Vec model, setting pretrained GoogleNews Vector\n",
    "        self._d2v = Doc2Vec(vector_size=vector_size, window=window, min_count=min_count, dm=dm, epochs=epochs,\n",
    "                            pretrained=self.pretrained)\n",
    "        # build vocabulary from corpus\n",
    "        self._d2v.build_vocab(self.docs.get_tagged())\n",
    "\n",
    "        # somehow, the training won't start automatically, and must be manually started\n",
    "        self._d2v.train(self.docs.get_tagged(), total_examples=self._d2v.corpus_count, epochs=epochs)\n",
    "\n",
    "        # list document embeddings by order of their tags\n",
    "        self._d2v_embedding = [self._d2v.docvecs[index]\n",
    "                               for index in range(len(self.docs.get_tagged()))]\n",
    "\n",
    "    def get_doc2vec(self, vectors_size=300, window=5, min_count=5, dm=1, epochs=20):\n",
    "        \"\"\"\n",
    "        get the doc2vec embeddings with word vectors pretrained on GoogleNews task\n",
    "        :param vectors_size: size for document embeddings, should be 300 if using GoogleNews pretrained word vectors\n",
    "        :param window: number of tokens to be include in both directions\n",
    "        :param min_count: lower threshold for a token to be included\n",
    "        :param dm: using distributed memory or not\n",
    "            if 1, use distributed memory\n",
    "            if 0, use distributed bag of words\n",
    "        :param epochs: number of epochs for training, usually < 20\n",
    "        :return: a list of document embeddings, vector size can be specified\n",
    "        \"\"\"\n",
    "        if vectors_size != 300:\n",
    "            print(\"Warning: pretrained Google News vecs have length 300, got vec-size={} \".format(vectors_size))\n",
    "\n",
    "        if not hasattr(self, '_d2v_embedding'):\n",
    "            self._set_doc2vec(vector_size=vectors_size, window=window, min_count=min_count, dm=dm, epochs=epochs)\n",
    "\n",
    "        return self._d2v_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods above are used as tools for preprocessing the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "# load the raw data set and Google pretrained w2v model\n",
    "df = pd.read_csv(\"./fake_or_real_news.csv\")\n",
    "pretrained = \"./\"\n",
    "# obtain the raw news texts and titles\n",
    "raw_texts = df['text'].values\n",
    "raw_title = df['title'].values\n",
    "\n",
    "# obtain the raw label data\n",
    "def trans_labels(labels):\n",
    "    for idx in range(len(labels)):\n",
    "        if labels[idx] == 'FAKE':\n",
    "            labels[idx] = 0\n",
    "        else:\n",
    "            labels[idx] = 1\n",
    "    return np.array(ytrain, dtype=int)\n",
    "\n",
    "labels = trans_labels(df['labels'].values)\n",
    "\n",
    "# build two instances for preprocessing raw data\n",
    "texts = DocumentSequence(raw_texts, clean=True, sw=stopwords.words('english'), punct=punctuation)\n",
    "titles = DocumentSequence(raw_title, clean=True, sw=stopwords.words('english'), punct=punctuation)\n",
    "\n",
    "# build two instances for producing document embeddings\n",
    "text_embedder = DocumentEmbedder(texts, pretrained_word2vec=pretrained)\n",
    "titles_embedder = DocumentEmbedder(titles, pretrained_word2vec=pretrained)\n",
    "\n",
    "# vectors_size: Number of dimensions for the embedding model\n",
    "# window: Number of context words to observe in each direction within a document\n",
    "# min_count: Minimum frequency for words included in model\n",
    "# dm (distributed memory): '0' indicates DBOW model; '1' indicates DM\n",
    "# epoches: Number of epochs to train the model for\n",
    "text_embeddings = text_embedder.get_doc2vec(vectors_size=300,\n",
    "                                            window=13,\n",
    "                                            min_count=5,\n",
    "                                            dm=0,\n",
    "                                            epochs=100)\n",
    "\n",
    "title_embeddings = titles_embedder.get_doc2vec(vectors_size=300,\n",
    "                                               window=13,\n",
    "                                               min_count=5,\n",
    "                                               dm=0,\n",
    "                                               epochs=100)\n",
    "\n",
    "# if the embeddings is in a list, stack them into a 2-D numpy array\n",
    "def trans_list_to_array(embeddings):\n",
    "    if isinstance(embeddings, list): \n",
    "        try:\n",
    "            embeddings = np.stack(emb if isinstance(emb, np.ndarray) else np.zeros(300) for emb in embeddings)\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "    return embeddings\n",
    "\n",
    "# change text_embeddings and title_embeddings into 2-D numpy array\n",
    "text_embeddings = trans_list_to_array(text_embeddings)\n",
    "title_embeddings = trans_list_to_array(title_embeddings)\n",
    "\n",
    "# concatenate text matrix and title matrix as a whole for training\n",
    "news_embeddings = np.concatenate((title_embeddings, text_embeddings), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process above is used to get embeddings of preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.model_selection._search import BaseSearchCV\n",
    "        \n",
    "# perform the split which gets us the train data and the test data\n",
    "news_train, news_test, labels_train, labels_test = train_test_split(news_embeddings, labels,\n",
    "                                                                    test_size=0.25,\n",
    "                                                                    random_state=0,\n",
    "                                                                    stratify=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process above is used to split the original data into two parts, one for training, the other for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from scipy.stats import randint\n",
    "from scipy.stats.distributions import uniform\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "# MLP classifier\n",
    "mlp = MLPClassifier(activation='relu', alpha=0.01, batch_size='auto', beta_1=0.8,\n",
    "                    beta_2=0.9, early_stopping=False, epsilon=1e-08,\n",
    "                    hidden_layer_sizes=(600, 300), learning_rate='constant',\n",
    "                    learning_rate_init=0.0001, max_iter=200, momentum=0.9,\n",
    "                    nesterovs_momentum=True, power_t=0.5, random_state=0, shuffle=True,\n",
    "                    solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
    "                    warm_start=False)\n",
    "\n",
    "# KNN classifier\n",
    "knn = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='cosine',\n",
    "                           metric_params=None, n_jobs=1, n_neighbors=10, p=2,\n",
    "                           weights='distance')\n",
    "\n",
    "# QDA classifier\n",
    "qda = QuadraticDiscriminantAnalysis(priors=array([0.5, 0.5]),\n",
    "                                    reg_param=0.6531083254653984, store_covariance=False,\n",
    "                                    PSTstore_covariances=None, tol=0.0001)\n",
    "\n",
    "# GDB classifier\n",
    "gdb = GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
    "                                 learning_rate=0.1, loss='exponential', max_depth=10,\n",
    "                                 max_features='log2', max_leaf_nodes=None,\n",
    "                                 min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                                 min_samples_leaf=0.0012436966435001434,\n",
    "                                 min_samples_split=100, min_weight_fraction_leaf=0.0,\n",
    "                                 n_estimators=200, presort='auto', random_state=0,\n",
    "                                 subsample=0.8, verbose=0, warm_start=False)\n",
    "\n",
    "# SVC classifier\n",
    "svc = SVC(C=0.8, cache_size=200, class_weight=None, coef0=0.0,\n",
    "          decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
    "          max_iter=-1, probability=False, random_state=0, shrinking=True,\n",
    "          tol=0.001, verbose=False)\n",
    "\n",
    "# GNB classifier\n",
    "gnb = GaussianNB(priors=None)\n",
    "\n",
    "# RF classifier\n",
    "rf = RandomForestClassifier(bootstrap=False, class_weight=None,\n",
    "                            criterion='entropy', max_depth=10, max_features=7,\n",
    "                            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                            min_impurity_split=None, min_samples_leaf=9,\n",
    "                            min_samples_split=6, min_weight_fraction_leaf=0.0,\n",
    "                            n_estimators=50, n_jobs=1, oob_score=False, random_state=None,\n",
    "                            verbose=0, warm_start=False)\n",
    "\n",
    "# All the parameters of the classifiers above are optimal in our experiments\n",
    "# The list below is used to store every classifier instance\n",
    "classifiers_list = [mlp, knn, qda, gdb, svc, gnb, rf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process above is used to build every classifier with almost optimal parameters in our experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![a](resources/models_with_best_performance.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# print details of testing results\n",
    "for model in classifiers_list:\n",
    "    model.fit(news_train, labels_train)\n",
    "    labels_pred = model.predict(news_test)\n",
    "    \n",
    "    # Report the metrics\n",
    "    target_names = ['Fake', 'Real']\n",
    "    print(classification_report(y_true=labels_test, y_pred=labels_pred, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
